---
date: "`r Sys.Date()`"
title: "Maximum Margin Interval Trees"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Maximum Margin Interval Trees}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Summary of the Maximum Margin Interval Trees (MMIT) project under the R project for statistical computing.

*Authors: Parismita Das, Alexandre Drouin, Torsten Hothorn and Toby Dylan Hocking*

---------------------------------------------------------------

## Table of contents

* [Introduction](#introduction)
* [Installation](#installation)
* [Tutorials](#tutorials)
  * [mmit](#mmit)
  * [mmit.predict](#mmitpredict)
  * [mmit.pruning](#mmitpruning)
  * [mmit.cv](#mmitcv)
  * [mmif](#mmif)
  * [mmif.predict](#mmifpredict)
  * [mmif.cv](#mmifcv)
  * [mse](#mse)
  * [zero_one_loss](#zero_one_loss)
  
 -----------------------------------------------------------------------------------------------------

## Introduction


There are few R packages available for interval regression, a machine learning problem 
which is important in genomics and medicine. Like usual regression, the goal is to learn 
a function that inputs a feature vector and outputs a real-valued prediction. 
Unlike usual regression, each response in the training set is an interval of acceptable 
values (rather than one value). In the terminology of the survival analysis literature, 
this is regression with “left, right, and interval censored” output/response data.

Max margin interval trees are a new type of nonlinear model for this problem ([Drouin et al., 2017](https://papers.nips.cc/paper/7080-maximum-margin-interval-trees)). A dynamic programming algorithm is used for computing the optimal solution inlog-linear time. We show empirically that this algorithm achieves state-of-the-art speed and prediction accuracy in a benchmark of several data sets.
This algorithm was implemented using the *partykit* library for creation of tree structure and better visualization.
 
Specifically, the following learning algorithms are implemented in the mmit package:
  * Max margin interval trees (mmit, mmit.predict, mmit.pruning, mmit.cv)
  * Max margin interval random forests (mmif, mmif.predict, mmif.cv)
  * Max margin interval Adaboost (in progress) 
  

For each algorithm, we implemented the following:
* Learning: input examples represented by their features and interval label and output a model (single party object or list of party objects)
* Cross-validation: grid seach k-fold cross-validation to select the hyperparameters
* Prediction: input examples represented by their features and output predictions
* Minimum cost-complexity pruning (for MMIT only): reduce the size of trees by cutting branches
 
-----------------------------------------------------------------------------------------------------------

## Installation

The `mmit` R package can be installed via the following R
commands.

```
if(!require(devtools))install.package("devtools")
devtools::install_github("aldro61/mmit/Rpackage")
```

----------------------------------------------------------------------------------------------------------

## Tutorials
  
Here is a tutorial on how to use the package.

### mmit()
  
This function is used to create the maximum margin interval trees. It returns the learned tree as a party object.

#### Usage:
  
`mmit(target.mat, feature.mat, max_depth = Inf, margin = 0, loss = "hinge",
min_sample = 1)`
  
#### Example:
  
```R
library(mmit)
target.mat <- rbind(
  c(0,1), c(0,1), c(0,1),
  c(2,3), c(2,3), c(2,3))

feature.mat <- rbind(
  c(1,0,0), c(1,1,0), c(1,2,0),
  c(1,3,0), c(1,4,0), c(1,5,0))

colnames(feature.mat) <- c("a", "b", "c")
feature.mat <- data.frame(feature.mat)


out <- mmit(target.mat, feature.mat)
```

#### Output:

```{r mmitplot, echo=FALSE}
library(mmit)
target.mat <- rbind(
  c(0,1), c(0,1), c(0,1),
  c(2,3), c(2,3), c(2,3))

feature.mat <- rbind(
  c(1,0,0), c(1,1,0), c(1,2,0),
  c(1,3,0), c(1,4,0), c(1,5,0))

colnames(feature.mat) <- c("a", "b", "c")
feature.mat <- data.frame(feature.mat)


out <- mmit(target.mat, feature.mat)
plot(out)
```

### mmit.predict()
  
  Fits the new data into the MMIT model to give prediction values
  
#### Usage:
  
`mmit.predict(tree, newdata = NULL, perm = NULL)`
  
#### Example:
  
```R
library(mmit)
target.mat <- rbind(
  c(0,1), c(0,1), c(0,1),
  c(2,3), c(2,3), c(2,3))

feature.mat <- rbind(
  c(1,0,0), c(1,1,0), c(1,2,0),
  c(1,3,0), c(1,4,0), c(1,5,0))

colnames(feature.mat) <- c("a", "b", "c")
feature.mat <- data.frame(feature.mat)

tree <- mmit(target.mat, feature.mat)
pred <- mmit.predict(tree)
```

#### Output:

```{r mmit.predictoutput, echo= FALSE}
library(mmit)
target.mat <- rbind(
  c(0,1), c(0,1), c(0,1),
  c(2,3), c(2,3), c(2,3))

feature.mat <- rbind(
  c(1,0,0), c(1,1,0), c(1,2,0),
  c(1,3,0), c(1,4,0), c(1,5,0))

colnames(feature.mat) <- c("a", "b", "c")
feature.mat <- data.frame(feature.mat)

tree <- mmit(target.mat, feature.mat)
pred <- mmit.predict(tree)
print(pred)
```

### mmit.pruning()
  
Pruning the regression tree for censored data to give all the alpha values and trees as output.
  
#### Usage:

`mmit.pruning(tree)`
  
#### Example:
  
```R
library(mmit)
target.mat <- rbind(
  c(0,1), c(0,1), c(0,1),
  c(2,3), c(2,3), c(2,3))

feature.mat <- rbind(
  c(1,0,0), c(1,1,0), c(1,2,0),
  c(1,3,0), c(1,4,0), c(1,5,0))

colnames(feature.mat) <- c("a", "b", "c")
feature.mat <- data.frame(feature.mat)


tree <- mmit(target.mat, feature.mat)
pruned_tree <- mmit.pruning(tree)
```

#### Output:


```{r mmit.pruningplot, echo=FALSE}
library(mmit)
target.mat <- rbind(
  c(0,1), c(0,1), c(0,1),
  c(2,3), c(2,3), c(2,3))

feature.mat <- rbind(
  c(1,0,0), c(1,1,0), c(1,2,0),
  c(1,3,0), c(1,4,0), c(1,5,0))

colnames(feature.mat) <- c("a", "b", "c")
feature.mat <- data.frame(feature.mat)


tree <- mmit(target.mat, feature.mat)
pruned_tree <- mmit.pruning(tree)

print("pruned tree for alpha = 0")
plot(pruned_tree[[1]]$tree)
print("pruned tree for alpha = 3")
plot(pruned_tree[[2]]$tree)
```

### mmit.cv()
  
Performing grid search to select the best parameters via cross validation on the a regression tree for censored data.
It outputs all the CV results, the best model and best parameters.
  
#### Usage:
  
`mmit.cv(target.mat, feature.mat, param_grid, n_folds = 3, scorer = NULL, pruning = TRUE)`
  
#### Example:
  
```R
library(mmit)
data(neuroblastomaProcessed, package="penaltyLearning")
feature.mat <- data.frame(neuroblastomaProcessed$feature.mat)[1:45,]
target.mat <- neuroblastomaProcessed$target.mat[1:45,]
  
param_grid <- NULL
param_grid$max_depth <- c(Inf, 4, 3, 2, 1, 0)
param_grid$margin <- c(2, 3, 5)
param_grid$min_sample <- c(2, 5, 10, 20)
param_grid$loss <- c("hinge", "square")
if(require(future)){ plan(multiprocess)}
result <- mmit.cv(target.mat, feature.mat, param_grid, scorer = mse)
```

#### Output:
```{r mmit.cv1, echo= FALSE}
library(mmit)
data(neuroblastomaProcessed, package="penaltyLearning")
feature.mat <- data.frame(neuroblastomaProcessed$feature.mat)[1:45,]
target.mat <- neuroblastomaProcessed$target.mat[1:45,]
  
param_grid <- NULL
param_grid$max_depth <- c(Inf, 4, 3, 2, 1, 0)
param_grid$margin <- c(2, 3, 5)
param_grid$min_sample <- c(2, 5, 10, 20)
param_grid$loss <- c("hinge", "square")
if(require(future)){ plan(multiprocess)}
result <- mmit.cv(target.mat, feature.mat, param_grid, scorer = mse)
#print("result: ")
#print(result)
```
### mmif()
  
Learning a random forest of Max Margin Interval Tree and giving list of trees as output.
  
#### Usage:

`mmif(target.mat, feature.mat, max_depth = Inf, margin = 0, loss = "hinge",
min_sample = 1, n_trees = 10,
n_features = ceiling(ncol(feature.mat)^0.5))`
  
#### Example:
  
```R
library(mmit)

  data(neuroblastomaProcessed, package="penaltyLearning")
  feature.mat <- data.frame(neuroblastomaProcessed$feature.mat)[1:45,]
  target.mat <- neuroblastomaProcessed$target.mat[1:45,]
  if(require(future)){ plan(multiprocess)}
  trees <- mmif(target.mat, feature.mat, max_depth = Inf, margin = 2.0, loss = "hinge", min_sample = 1)
```

#### Output:
```{r mmif, echo=FALSE}
library(mmit)

  data(neuroblastomaProcessed, package="penaltyLearning")
  feature.mat <- data.frame(neuroblastomaProcessed$feature.mat)[1:45,]
  target.mat <- neuroblastomaProcessed$target.mat[1:45,]
  if(require(future)){ plan(multiprocess)}
  trees <- mmif(target.mat, feature.mat, max_depth = Inf, margin = 2.0, loss = "hinge", min_sample = 1)
  lapply(trees, plot)
```
### mmif.predict()
  
Predictions with random forests of Max Margin Interval Trees

#### Usage:

`mmif.predict(forest, test_feature.mat = NULL)`

#### Example:
  
```R
library(mmit)

target.mat <- rbind(
  c(0,1), c(0,1), c(0,1),
  c(2,3), c(2,3), c(2,3))

feature.mat <- rbind(
  c(1,0,0), c(1,1,0), c(1,2,0),
  c(1,3,0), c(1,4,0), c(1,5,0))

colnames(feature.mat) <- c("a", "b", "c")
feature.mat <- data.frame(feature.mat)

forest <- mmif(target.mat, feature.mat)
pred <- mmif.predict(forest, feature.mat)
```

#### Output:
```{r mmif.predict, echo=FALSE}
library(mmit)

target.mat <- rbind(
  c(0,1), c(0,1), c(0,1),
  c(2,3), c(2,3), c(2,3))

feature.mat <- rbind(
  c(1,0,0), c(1,1,0), c(1,2,0),
  c(1,3,0), c(1,4,0), c(1,5,0))

colnames(feature.mat) <- c("a", "b", "c")
feature.mat <- data.frame(feature.mat)

forest <- mmif(target.mat, feature.mat)
pred <- mmif.predict(forest, feature.mat)
print(pred)
```

### mmif.cv()
  
Performing grid search to select the best hyperparameters of mmif via cross-validation.
  
#### Usage:

`mmif.cv(target.mat, feature.mat, param_grid, n_folds = 3, scorer = NULL,
n_cpu = 1)`
  
#### Example:
  
```R
library(mmit)

data(neuroblastomaProcessed, package="penaltyLearning")
feature.mat <- data.frame(neuroblastomaProcessed$feature.mat)[1:45,]
target.mat <- neuroblastomaProcessed$target.mat[1:45,]
 
param_grid <- NULL
param_grid$max_depth <- c(4, 3)
param_grid$margin <- c(2, 3)
param_grid$min_sample <- c(5, 20)
param_grid$loss <- c("hinge", "square")
param_grid$n_trees <- c(10)
param_grid$n_features <- c(as.integer(ncol(feature.mat)**0.5))
if(require(future)){ plan(multiprocess)}
result <- mmif.cv(target.mat, feature.mat, param_grid, scorer = mse, n_cpu = 4)
```

#### Output:
```{r mmif.cv, echo=FALSE}
library(mmit)

target.mat <- rbind(
  c(0,1), c(0,1), c(0,1),
  c(2,3), c(2,3), c(2,3))

feature.mat <- rbind(
  c(1,0,0), c(1,1,0), c(1,2,0),
  c(1,3,0), c(1,4,0), c(1,5,0))

colnames(feature.mat) <- c("a", "b", "c")
feature.mat <- data.frame(feature.mat)

forest <- mmif(target.mat, feature.mat)
if(require(future)){ plan(multiprocess)}
pred <- mmif.predict(forest, feature.mat)
print(pred)
```

### mse()
  
Calculation of the mean square error for intervals.
  
#### Usage:

`mse(y_true, y_pred)`

#### Example:
  
```R
library(mmit)
y_true <- rbind(
  c(0,1), c(0,1), c(0,1),
  c(2,3), c(2,3), c(2,3))

y_pred <- c(0.5, 2, 0, 1.5, 3.5, 2.5)

out <- mse(y_true, y_pred)
```

#### Output:
```{r mse, echo= FALSE}
library(mmit)
y_true <- rbind(
  c(0,1), c(0,1), c(0,1),
  c(2,3), c(2,3), c(2,3))

y_pred <- c(0.5, 2, 0, 1.5, 3.5, 2.5)

out <- mse(y_true, y_pred)
print(out)
```

### zero_one_loss()
  
Calculation of the zero-one loss for interval, i.e., zero error if the prediction is inside the interval and one error if it is ouside.
  
#### Usage:

`zero_one_loss(y_true, y_pred)`
  
#### Example:
  
```R
library(mmit)
y_true <- rbind(
  c(0,1), c(0,1), c(0,1),
  c(2,3), c(2,3), c(2,3))

y_pred <- c(0.5, 2, 0, 1.5, 3.5, 2.5)

out <- zero_one_loss(y_true, y_pred)
```

#### Output:

```{r zero, echo=FALSE}
library(mmit)
y_true <- rbind(
  c(0,1), c(0,1), c(0,1),
  c(2,3), c(2,3), c(2,3))

y_pred <- c(0.5, 2, 0, 1.5, 3.5, 2.5)

out <- zero_one_loss(y_true, y_pred)
print(out)
```


